1:"$Sreact.fragment"
2:I[2260,["215","static/chunks/215-388a4da7c5323a15.js","177","static/chunks/app/layout-325168dde6378f38.js"],"default"]
3:I[912,["215","static/chunks/215-388a4da7c5323a15.js","177","static/chunks/app/layout-325168dde6378f38.js"],"default"]
4:I[9766,[],""]
5:I[8924,[],""]
6:I[2619,["619","static/chunks/619-ba102abea3e3d0e4.js","215","static/chunks/215-388a4da7c5323a15.js","356","static/chunks/356-1fcf197c11312f1f.js","953","static/chunks/app/blog/%5Bslug%5D/page-1df3de982003a2e7.js"],""]
7:I[979,["215","static/chunks/215-388a4da7c5323a15.js","177","static/chunks/app/layout-325168dde6378f38.js"],"default"]
f:I[7150,[],""]
:HL["/_next/static/css/3499fdfcb29162bf.css","style"]
:HL["/_next/static/css/5063fcb600605bcd.css","style"]
0:{"P":null,"b":"mqoiKBG41VDaIdpl--yaH","p":"","c":["","blog","llm-starter-guide",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","llm-starter-guide","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/3499fdfcb29162bf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"dark scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"href":"https://api.fontshare.com/v2/css?f[]=clash-display@400,500,600,700&f[]=satoshi@300,400,500,700,900&display=swap","rel":"stylesheet"}],["$","link",null,{"href":"https://fonts.googleapis.com/css2?family=DM+Serif+Text:ital@0;1&display=swap","rel":"stylesheet"}]]}],["$","body",null,{"className":"flex flex-col min-h-screen overflow-x-hidden","style":{"backgroundColor":"var(--bg-primary)","color":"var(--text-primary)"},"children":[["$","$L2",null,{}],["$","div",null,{"className":"fixed inset-0 pointer-events-none z-50 opacity-[0.03] mix-blend-overlay","style":{"backgroundImage":"url(\"data:image/svg+xml,%3Csvg viewBox='0 0 200 200' xmlns='http://www.w3.org/2000/svg'%3E%3Cfilter id='noiseFilter'%3E%3CfeTurbulence type='fractalNoise' baseFrequency='0.65' numOctaves='3' stitchTiles='stitch'/%3E%3C/filter%3E%3Crect width='100%25' height='100%25' filter='url(%23noiseFilter)'/%3E%3C/svg%3E\")"}}],["$","$L3",null,{}],["$","main",null,{"className":"max-w-4xl mx-auto px-6 pt-24 pb-12 flex-grow w-full","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","div",null,{"className":"flex flex-col items-center justify-center min-h-[400px] gap-6 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold","style":{"color":"var(--text-primary)"},"children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold","style":{"color":"var(--text-secondary)"},"children":"Page Not Found"}],["$","p",null,{"style":{"color":"var(--text-muted)"},"children":"The page you're looking for doesn't exist."}],["$","$L6",null,{"href":"/","className":"mt-4 px-6 py-3 font-medium transition-colors","style":{"backgroundColor":"var(--accent)","color":"var(--bg-primary)"},"children":"Back to Home"}]]}],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"border-t mt-12","style":{"borderColor":"var(--bg-border)","backgroundColor":"var(--bg-secondary)"},"children":["$","div",null,{"className":"max-w-4xl mx-auto px-6 py-12","children":[["$","div",null,{"className":"grid grid-cols-1 md:grid-cols-3 gap-8 mb-8","children":[["$","div",null,{"children":[["$","h3",null,{"className":"text-lg font-bold mb-2","style":{"color":"var(--accent)"},"children":"BlackCatDesigns"}],["$","p",null,{"className":"text-sm","style":{"color":"var(--text-muted)"},"children":"Cultivating Aesthetic Transformations"}]]}],["$","div",null,{"children":[["$","h4",null,{"className":"text-sm font-semibold mb-4","style":{"color":"var(--text-secondary)"},"children":"Navigation"}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"children":["$","$L7",null,{"href":"/","children":"Home"}]}],["$","li",null,{"children":["$","$L7",null,{"href":"/projects","children":"Projects"}]}],["$","li",null,{"children":["$","$L7",null,{"href":"/blog","children":"Blog"}]}],["$","li",null,{"children":["$","$L7",null,{"href":"/about","children":"About"}]}],["$","li",null,{"children":["$","$L7",null,{"href":"/contact","children":"Contact"}]}]]}]]}],["$","div",null,{"children":[["$","h4",null,{"className":"text-sm font-semibold mb-4","style":{"color":"var(--text-secondary)"},"children":"Connect"}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"children":["$","$L7",null,{"href":"https://github.com/theblackcat98","external":true,"children":["$","span",null,{"className":"flex items-center gap-2","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-github w-4 h-4","aria-hidden":"true","children":[["$","path","tonef",{"d":"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"}],"$L8","$undefined"]}],"GitHub"]}]}]}],"$L9"]}]]}]]}],"$La"]}]}]]}]]}]]}],{"children":["blog","$Lb",{"children":[["slug","llm-starter-guide","d"],"$Lc",{"children":["__PAGE__","$Ld",{},null,false]},null,false]},null,false]},null,false],"$Le",false]],"m":"$undefined","G":["$f",[]],"s":false,"S":true}
11:I[4431,[],"OutletBoundary"]
13:I[5278,[],"AsyncMetadataOutlet"]
15:I[4431,[],"ViewportBoundary"]
17:I[4431,[],"MetadataBoundary"]
18:"$Sreact.suspense"
8:["$","path","9comsn",{"d":"M9 18c-4.51 2-5-2-7-2"}]
9:["$","li",null,{"children":["$","$L7",null,{"href":"https://instagram.com/theblackcat98","external":true,"children":["$","span",null,{"className":"flex items-center gap-2","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-instagram w-4 h-4","aria-hidden":"true","children":[["$","rect","2e1cvw",{"width":"20","height":"20","x":"2","y":"2","rx":"5","ry":"5"}],["$","path","9exkf1",{"d":"M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"}],["$","line","r4j83e",{"x1":"17.5","x2":"17.51","y1":"6.5","y2":"6.5"}],"$undefined"]}],"Instagram"]}]}]}]
a:["$","div",null,{"className":"border-t pt-6 text-center text-sm","style":{"borderColor":"var(--bg-border)","color":"var(--text-muted)"},"children":["$","p",null,{"children":"¬© 2025 BlackCatDesigns. All rights reserved."}]}]
b:["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
c:["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
d:["$","$1","c",{"children":["$L10",[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/5063fcb600605bcd.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$L11",null,{"children":["$L12",["$","$L13",null,{"promise":"$@14"}]]}]]}]
e:["$","$1","h",{"children":[null,[["$","$L15",null,{"children":"$L16"}],null],["$","$L17",null,{"children":["$","div",null,{"hidden":true,"children":["$","$18",null,{"fallback":null,"children":"$L19"}]}]}]]}]
1a:I[6659,["619","static/chunks/619-ba102abea3e3d0e4.js","215","static/chunks/215-388a4da7c5323a15.js","356","static/chunks/356-1fcf197c11312f1f.js","953","static/chunks/app/blog/%5Bslug%5D/page-1df3de982003a2e7.js"],"default"]
1b:I[5707,["619","static/chunks/619-ba102abea3e3d0e4.js","215","static/chunks/215-388a4da7c5323a15.js","356","static/chunks/356-1fcf197c11312f1f.js","953","static/chunks/app/blog/%5Bslug%5D/page-1df3de982003a2e7.js"],"default"]
1c:I[2514,["619","static/chunks/619-ba102abea3e3d0e4.js","215","static/chunks/215-388a4da7c5323a15.js","356","static/chunks/356-1fcf197c11312f1f.js","953","static/chunks/app/blog/%5Bslug%5D/page-1df3de982003a2e7.js"],"default"]
1d:I[8655,["619","static/chunks/619-ba102abea3e3d0e4.js","215","static/chunks/215-388a4da7c5323a15.js","356","static/chunks/356-1fcf197c11312f1f.js","953","static/chunks/app/blog/%5Bslug%5D/page-1df3de982003a2e7.js"],"default"]
1e:T1ab1,<h1>Getting Started with Local LLMs: 2025 Beginner‚Äôs Guide</h1>
<p>Running large language models locally has never been more accessible. Whether you‚Äôre experimenting, building private apps, or just curious, you can now get state-of-the-art performance without sending your data to the cloud.</p>
<p>This guide cuts through the noise and gives you exactly what you need to get started in 2025 ‚Äì with clear recommendations, hardware reality checks, and practical commands.</p>
<h2 id="tl-dr-the-fast-path">‚ö° TL;DR ‚Äì The Fast Path</h2>
<p><strong>Easiest start</strong> ‚Üí <strong>Ollama</strong><br>
<strong>Maximum efficiency &#x26; control</strong> ‚Üí <strong>llama.cpp</strong><br>
<strong>High-throughput serving</strong> ‚Üí <strong>vLLM</strong> or <strong>Hugging Face TGI</strong><br>
<strong>Single-GPU API</strong> ‚Üí <strong>TabbyAPI</strong></p>
<p><strong>Best starter models (2025):</strong></p>
<ul>
<li>Qwen3-4B ‚Üí tiny but surprisingly capable</li>
<li>Qwen3-14B ‚Üí excellent reasoning</li>
<li>gpt-oss (MoE) ‚Üí efficiency king for long contexts</li>
</ul>
<p><strong>Hardware cheat sheet (quantized):</strong></p>
<ul>
<li>8‚Äì12 GB VRAM ‚Üí 4B‚Äì8B models</li>
<li>16‚Äì24 GB VRAM ‚Üí 14B‚Äì30B models</li>
<li>40+ GB VRAM ‚Üí 70B+ models</li>
</ul>
<p>Got <strong>20 GB VRAM + 32 GB RAM</strong>? You‚Äôre in the sweet spot for 14B at 32k context or 4B at 128k.</p>
<h2 id="3-step-quick-start-takes-x3c-10-minutes">üßµ 3-Step Quick Start (Takes &#x3C;10 Minutes)</h2>
<pre data-language="bash"><code class="language-bash"># 1. Install Ollama (macOS, Windows, Linux)
# Visit https://ollama.com and download

# 2. Pull and run a great starter model
ollama pull qwen3:4b
ollama run qwen3:4b

# 3. Want longer context? (if the model supports YaRN/RoPE scaling)
ollama run qwen3:4b --num_ctx 32768
</code></pre>
<p>That‚Äôs it. You‚Äôre now running a local LLM.</p>
<h2 id="choose-your-inference-engine">üñ• Choose Your Inference Engine</h2>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Best For</th>
<th>Difficulty</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Ollama</strong></td>
<td>Beginners, quick chat, prototyping</td>
<td>‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ</td>
<td>Simple CLI + API, works with OpenWebUI</td>
</tr>
<tr>
<td><strong>llama.cpp</strong></td>
<td>Max speed &#x26; control, edge devices</td>
<td>‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ</td>
<td>Extremely efficient, huge community</td>
</tr>
<tr>
<td><strong>vLLM</strong></td>
<td>High-throughput GPU serving</td>
<td>‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
<td>PagedAttention = more tokens/sec</td>
</tr>
<tr>
<td><strong>Hugging Face TGI</strong></td>
<td>Production-grade API servers</td>
<td>‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
<td>OpenAI-compatible, great multi-GPU</td>
</tr>
<tr>
<td><strong>TabbyAPI</strong></td>
<td>Lightweight single-GPU API</td>
<td>‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ</td>
<td>Fast setup, works great with OpenWebUI</td>
</tr>
<tr>
<td><strong>TensorRT-LLM</strong></td>
<td>Peak NVIDIA performance</td>
<td>‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
<td>Complex but fastest on RTX 40/H100</td>
</tr>
</tbody>
</table>
<p>Start with <strong>Ollama</strong>. Graduate to <strong>llama.cpp</strong> or <strong>vLLM</strong> when you outgrow it.</p>
<h2 id="recommended-frontends-optional-but-nice">üñº Recommended Frontends (Optional but Nice)</h2>
<ul>
<li><strong>OpenWebUI</strong> ‚Äì Beautiful browser interface (works with Ollama, TabbyAPI, vLLM)</li>
<li><strong>LM Studio</strong> ‚Äì Excellent model manager + local OpenAI-compatible server</li>
<li><strong>Jan</strong> ‚Äì Clean, cross-platform, open-source</li>
</ul>
<h2 id="hardware-reality-check-2025">‚öôÔ∏è Hardware Reality Check (2025)</h2>
<table>
<thead>
<tr>
<th>Resource</th>
<th>Minimum</th>
<th>Recommended</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPU</td>
<td>RTX 3060 12 GB</td>
<td>RTX 4090 / A6000+</td>
<td>NVIDIA dominates local inference</td>
</tr>
<tr>
<td>VRAM</td>
<td>8 GB</td>
<td>24 GB+</td>
<td>The #1 bottleneck</td>
</tr>
<tr>
<td>System RAM</td>
<td>16 GB</td>
<td>32‚Äì64 GB</td>
<td>Needed for KV cache spillover</td>
</tr>
<tr>
<td>Storage</td>
<td>50 GB free</td>
<td>NVMe SSD</td>
<td>Quantized models: 2‚Äì50 GB each</td>
</tr>
</tbody>
</table>
<p><strong>Pro tip</strong>: Mixture-of-Experts (MoE) models like Mixtral 8x7B activate only 7B parameters per token, gpt-oss activates ~1.7B ‚Üí they punch way above their weight on hybrid CPU+GPU setups.</p>
<h2 id="quick-vocabulary-you-ll-hear-these-terms">üìñ Quick Vocabulary (You‚Äôll Hear These Terms)</h2>
<ul>
<li><strong>GGUF</strong> ‚Äì The go-to format for quantized models (used by Ollama &#x26; llama.cpp)</li>
<li><strong>Quantization</strong> ‚Äì Compressing model weights (Q4_K_M = great balance)</li>
<li><strong>Context</strong> ‚Äì How much text the model can ‚Äúsee‚Äù at once</li>
<li><strong>KV Cache</strong> ‚Äì Memory that grows with context length (VRAM eater)</li>
<li><strong>YaRN / RoPE Scaling</strong> ‚Äì Tricks to extend context beyond original training</li>
<li><strong>MoE</strong> ‚Äì Only a fraction of the model runs per token ‚Üí efficient</li>
<li><strong>PagedAttention</strong> ‚Äì vLLM‚Äôs secret sauce for high throughput</li>
</ul>
<h2 id="context-vs-vram-cheat-sheet-q4-k-m-quantized">üìè Context vs VRAM Cheat Sheet (Q4_K_M Quantized)</h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>32k Context</th>
<th>64k Context</th>
<th>128k Context</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen3-4B</td>
<td>5‚Äì6 GB</td>
<td>10‚Äì12 GB</td>
<td>20‚Äì24 GB</td>
<td>Perfect for modest GPUs</td>
</tr>
<tr>
<td>Qwen3-14B</td>
<td>16‚Äì20 GB</td>
<td>32‚Äì38 GB</td>
<td>Not advised</td>
<td>Sweet spot at 32k</td>
</tr>
<tr>
<td>gpt-oss (MoE)</td>
<td>10‚Äì12 GB</td>
<td>20‚Äì24 GB</td>
<td>40‚Äì48 GB</td>
<td>Best efficiency for long contexts</td>
</tr>
</tbody>
</table>
<h2 id="practical-tips-for-your-rig-20-gb-vram-32-gb-ram-example">üîß Practical Tips for Your Rig (20 GB VRAM + 32 GB RAM Example)</h2>
<ul>
<li>Run <strong>Qwen3-14B</strong> comfortably at 32k context</li>
<li>Use <strong>Qwen3-4B</strong> when you need 64k‚Äì128k</li>
<li>Pick <strong>gpt-oss</strong> for the best long-context efficiency</li>
<li>Default to <strong>Q4_K_M</strong> quantization ‚Äì best quality/size tradeoff</li>
<li>Most backends support OpenAI-compatible APIs ‚Üí swap tools by changing a URL</li>
</ul>
<h2 id="your-next-steps">‚úÖ Your Next Steps</h2>
<ol>
<li>Install <strong>Ollama</strong> (or your chosen engine)</li>
<li>Try <strong>qwen3:4b</strong> or <strong>qwen3:14b</strong></li>
<li>Experiment with <code>--num_ctx</code> for longer context</li>
<li>Add <strong>OpenWebUI</strong> or <strong>LM Studio</strong> for a nicer experience</li>
</ol>
<p>Local LLMs are evolving fast ‚Äì new models, better quantization, and longer contexts drop almost monthly. Bookmark this guide and check back.</p>
10:[["$","$L1a",null,{}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"headline\":\"Getting Started with Local LLMs: 2025 Beginner‚Äôs Guide\",\"description\":\"A clear, no-nonsense guide to running powerful large language models on your own hardware in 2025 ‚Äì perfect for beginners and power users alike.\",\"datePublished\":\"2025-11-28\",\"dateModified\":\"2025-11-28\",\"author\":{\"@type\":\"Person\",\"name\":\"BlackCatDesigns\",\"url\":\"https://github.com/theblackcat98\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"BlackCatDesigns\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https://blackcatdesigns.dev/logo.png\"}},\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https://blackcatdesigns.dev/blog/llm-starter-guide\"},\"keywords\":\"local-llm, ai, ollama, llama-cpp, self-hosted, 2025\"}"}}],["$","$L1b",null,{}],["$","div",null,{"className":"BlogLayout_container__5udUg","children":[["$","div",null,{"className":"BlogLayout_headerImage__c9chK","children":""}],["$","main",null,{"className":"BlogLayout_mainContent__kROAo","children":[["$","$L1c",null,{"children":["$","header",null,{"className":"mb-12 text-center","children":[["$","$L6",null,{"href":"/blog","className":"text-[#FFA89C] hover:text-[#FFB8A3] mb-6 inline-block transition-colors","children":"‚Üê Back to blog"}],["$","h1",null,{"className":"text-4xl md:text-6xl font-bold mb-6 text-gray-100 leading-tight","children":"Getting Started with Local LLMs: 2025 Beginner‚Äôs Guide"}],["$","div",null,{"className":"flex items-center justify-center gap-3 flex-wrap mb-6 text-gray-400","children":[["$","time",null,{"children":"November 27, 2025"}],["$","span",null,{"children":["¬∑ ",4," min read"]}],["$","span",null,{"children":["¬∑ by ","theblackcat"]}]]}],["$","div",null,{"className":"flex justify-center gap-2 flex-wrap","children":[["$","$L6","local-llm",{"href":"/blog/tag/local-llm","className":"text-sm px-3 py-1 border transition-colors hover:border-[#FFA89C] hover:text-[#FFA89C]","style":{"backgroundColor":"var(--bg-tertiary)","color":"var(--text-secondary)","borderColor":"var(--bg-border)","borderRadius":"var(--radius-md)"},"children":"local-llm"}],["$","$L6","ai",{"href":"/blog/tag/ai","className":"text-sm px-3 py-1 border transition-colors hover:border-[#FFA89C] hover:text-[#FFA89C]","style":{"backgroundColor":"var(--bg-tertiary)","color":"var(--text-secondary)","borderColor":"var(--bg-border)","borderRadius":"var(--radius-md)"},"children":"ai"}],["$","$L6","ollama",{"href":"/blog/tag/ollama","className":"text-sm px-3 py-1 border transition-colors hover:border-[#FFA89C] hover:text-[#FFA89C]","style":{"backgroundColor":"var(--bg-tertiary)","color":"var(--text-secondary)","borderColor":"var(--bg-border)","borderRadius":"var(--radius-md)"},"children":"ollama"}],["$","$L6","llama-cpp",{"href":"/blog/tag/llama-cpp","className":"text-sm px-3 py-1 border transition-colors hover:border-[#FFA89C] hover:text-[#FFA89C]","style":{"backgroundColor":"var(--bg-tertiary)","color":"var(--text-secondary)","borderColor":"var(--bg-border)","borderRadius":"var(--radius-md)"},"children":"llama-cpp"}],["$","$L6","self-hosted",{"href":"/blog/tag/self-hosted","className":"text-sm px-3 py-1 border transition-colors hover:border-[#FFA89C] hover:text-[#FFA89C]","style":{"backgroundColor":"var(--bg-tertiary)","color":"var(--text-secondary)","borderColor":"var(--bg-border)","borderRadius":"var(--radius-md)"},"children":"self-hosted"}],["$","$L6","2025",{"href":"/blog/tag/2025","className":"text-sm px-3 py-1 border transition-colors hover:border-[#FFA89C] hover:text-[#FFA89C]","style":{"backgroundColor":"var(--bg-tertiary)","color":"var(--text-secondary)","borderColor":"var(--bg-border)","borderRadius":"var(--radius-md)"},"children":"2025"}]]}]]}]}],["$","$L1d",null,{"html":"$1e"}],"$L1f","$L20","$L21","$L22"]}]]}]]
24:I[7154,["619","static/chunks/619-ba102abea3e3d0e4.js","215","static/chunks/215-388a4da7c5323a15.js","356","static/chunks/356-1fcf197c11312f1f.js","953","static/chunks/app/blog/%5Bslug%5D/page-1df3de982003a2e7.js"],"default"]
25:I[1356,["619","static/chunks/619-ba102abea3e3d0e4.js","215","static/chunks/215-388a4da7c5323a15.js","356","static/chunks/356-1fcf197c11312f1f.js","953","static/chunks/app/blog/%5Bslug%5D/page-1df3de982003a2e7.js"],"Image"]
23:T1ab1,<h1>Getting Started with Local LLMs: 2025 Beginner‚Äôs Guide</h1>
<p>Running large language models locally has never been more accessible. Whether you‚Äôre experimenting, building private apps, or just curious, you can now get state-of-the-art performance without sending your data to the cloud.</p>
<p>This guide cuts through the noise and gives you exactly what you need to get started in 2025 ‚Äì with clear recommendations, hardware reality checks, and practical commands.</p>
<h2 id="tl-dr-the-fast-path">‚ö° TL;DR ‚Äì The Fast Path</h2>
<p><strong>Easiest start</strong> ‚Üí <strong>Ollama</strong><br>
<strong>Maximum efficiency &#x26; control</strong> ‚Üí <strong>llama.cpp</strong><br>
<strong>High-throughput serving</strong> ‚Üí <strong>vLLM</strong> or <strong>Hugging Face TGI</strong><br>
<strong>Single-GPU API</strong> ‚Üí <strong>TabbyAPI</strong></p>
<p><strong>Best starter models (2025):</strong></p>
<ul>
<li>Qwen3-4B ‚Üí tiny but surprisingly capable</li>
<li>Qwen3-14B ‚Üí excellent reasoning</li>
<li>gpt-oss (MoE) ‚Üí efficiency king for long contexts</li>
</ul>
<p><strong>Hardware cheat sheet (quantized):</strong></p>
<ul>
<li>8‚Äì12 GB VRAM ‚Üí 4B‚Äì8B models</li>
<li>16‚Äì24 GB VRAM ‚Üí 14B‚Äì30B models</li>
<li>40+ GB VRAM ‚Üí 70B+ models</li>
</ul>
<p>Got <strong>20 GB VRAM + 32 GB RAM</strong>? You‚Äôre in the sweet spot for 14B at 32k context or 4B at 128k.</p>
<h2 id="3-step-quick-start-takes-x3c-10-minutes">üßµ 3-Step Quick Start (Takes &#x3C;10 Minutes)</h2>
<pre data-language="bash"><code class="language-bash"># 1. Install Ollama (macOS, Windows, Linux)
# Visit https://ollama.com and download

# 2. Pull and run a great starter model
ollama pull qwen3:4b
ollama run qwen3:4b

# 3. Want longer context? (if the model supports YaRN/RoPE scaling)
ollama run qwen3:4b --num_ctx 32768
</code></pre>
<p>That‚Äôs it. You‚Äôre now running a local LLM.</p>
<h2 id="choose-your-inference-engine">üñ• Choose Your Inference Engine</h2>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Best For</th>
<th>Difficulty</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Ollama</strong></td>
<td>Beginners, quick chat, prototyping</td>
<td>‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ</td>
<td>Simple CLI + API, works with OpenWebUI</td>
</tr>
<tr>
<td><strong>llama.cpp</strong></td>
<td>Max speed &#x26; control, edge devices</td>
<td>‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ</td>
<td>Extremely efficient, huge community</td>
</tr>
<tr>
<td><strong>vLLM</strong></td>
<td>High-throughput GPU serving</td>
<td>‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
<td>PagedAttention = more tokens/sec</td>
</tr>
<tr>
<td><strong>Hugging Face TGI</strong></td>
<td>Production-grade API servers</td>
<td>‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
<td>OpenAI-compatible, great multi-GPU</td>
</tr>
<tr>
<td><strong>TabbyAPI</strong></td>
<td>Lightweight single-GPU API</td>
<td>‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ</td>
<td>Fast setup, works great with OpenWebUI</td>
</tr>
<tr>
<td><strong>TensorRT-LLM</strong></td>
<td>Peak NVIDIA performance</td>
<td>‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
<td>Complex but fastest on RTX 40/H100</td>
</tr>
</tbody>
</table>
<p>Start with <strong>Ollama</strong>. Graduate to <strong>llama.cpp</strong> or <strong>vLLM</strong> when you outgrow it.</p>
<h2 id="recommended-frontends-optional-but-nice">üñº Recommended Frontends (Optional but Nice)</h2>
<ul>
<li><strong>OpenWebUI</strong> ‚Äì Beautiful browser interface (works with Ollama, TabbyAPI, vLLM)</li>
<li><strong>LM Studio</strong> ‚Äì Excellent model manager + local OpenAI-compatible server</li>
<li><strong>Jan</strong> ‚Äì Clean, cross-platform, open-source</li>
</ul>
<h2 id="hardware-reality-check-2025">‚öôÔ∏è Hardware Reality Check (2025)</h2>
<table>
<thead>
<tr>
<th>Resource</th>
<th>Minimum</th>
<th>Recommended</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPU</td>
<td>RTX 3060 12 GB</td>
<td>RTX 4090 / A6000+</td>
<td>NVIDIA dominates local inference</td>
</tr>
<tr>
<td>VRAM</td>
<td>8 GB</td>
<td>24 GB+</td>
<td>The #1 bottleneck</td>
</tr>
<tr>
<td>System RAM</td>
<td>16 GB</td>
<td>32‚Äì64 GB</td>
<td>Needed for KV cache spillover</td>
</tr>
<tr>
<td>Storage</td>
<td>50 GB free</td>
<td>NVMe SSD</td>
<td>Quantized models: 2‚Äì50 GB each</td>
</tr>
</tbody>
</table>
<p><strong>Pro tip</strong>: Mixture-of-Experts (MoE) models like Mixtral 8x7B activate only 7B parameters per token, gpt-oss activates ~1.7B ‚Üí they punch way above their weight on hybrid CPU+GPU setups.</p>
<h2 id="quick-vocabulary-you-ll-hear-these-terms">üìñ Quick Vocabulary (You‚Äôll Hear These Terms)</h2>
<ul>
<li><strong>GGUF</strong> ‚Äì The go-to format for quantized models (used by Ollama &#x26; llama.cpp)</li>
<li><strong>Quantization</strong> ‚Äì Compressing model weights (Q4_K_M = great balance)</li>
<li><strong>Context</strong> ‚Äì How much text the model can ‚Äúsee‚Äù at once</li>
<li><strong>KV Cache</strong> ‚Äì Memory that grows with context length (VRAM eater)</li>
<li><strong>YaRN / RoPE Scaling</strong> ‚Äì Tricks to extend context beyond original training</li>
<li><strong>MoE</strong> ‚Äì Only a fraction of the model runs per token ‚Üí efficient</li>
<li><strong>PagedAttention</strong> ‚Äì vLLM‚Äôs secret sauce for high throughput</li>
</ul>
<h2 id="context-vs-vram-cheat-sheet-q4-k-m-quantized">üìè Context vs VRAM Cheat Sheet (Q4_K_M Quantized)</h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>32k Context</th>
<th>64k Context</th>
<th>128k Context</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen3-4B</td>
<td>5‚Äì6 GB</td>
<td>10‚Äì12 GB</td>
<td>20‚Äì24 GB</td>
<td>Perfect for modest GPUs</td>
</tr>
<tr>
<td>Qwen3-14B</td>
<td>16‚Äì20 GB</td>
<td>32‚Äì38 GB</td>
<td>Not advised</td>
<td>Sweet spot at 32k</td>
</tr>
<tr>
<td>gpt-oss (MoE)</td>
<td>10‚Äì12 GB</td>
<td>20‚Äì24 GB</td>
<td>40‚Äì48 GB</td>
<td>Best efficiency for long contexts</td>
</tr>
</tbody>
</table>
<h2 id="practical-tips-for-your-rig-20-gb-vram-32-gb-ram-example">üîß Practical Tips for Your Rig (20 GB VRAM + 32 GB RAM Example)</h2>
<ul>
<li>Run <strong>Qwen3-14B</strong> comfortably at 32k context</li>
<li>Use <strong>Qwen3-4B</strong> when you need 64k‚Äì128k</li>
<li>Pick <strong>gpt-oss</strong> for the best long-context efficiency</li>
<li>Default to <strong>Q4_K_M</strong> quantization ‚Äì best quality/size tradeoff</li>
<li>Most backends support OpenAI-compatible APIs ‚Üí swap tools by changing a URL</li>
</ul>
<h2 id="your-next-steps">‚úÖ Your Next Steps</h2>
<ol>
<li>Install <strong>Ollama</strong> (or your chosen engine)</li>
<li>Try <strong>qwen3:4b</strong> or <strong>qwen3:14b</strong></li>
<li>Experiment with <code>--num_ctx</code> for longer context</li>
<li>Add <strong>OpenWebUI</strong> or <strong>LM Studio</strong> for a nicer experience</li>
</ol>
<p>Local LLMs are evolving fast ‚Äì new models, better quantization, and longer contexts drop almost monthly. Bookmark this guide and check back.</p>
1f:["$","$L1c",null,{"children":["$","div",null,{"className":"prose max-w-none prose-invert prose-lg","dangerouslySetInnerHTML":{"__html":"$23"}}]}]
20:["$","$L1c",null,{"children":["$","div",null,{"className":"mt-16 pt-8 border-t border-gray-800","children":["$","$L24",null,{"url":"https://blackcatdesigns.dev/blog/llm-starter-guide","title":"Getting Started with Local LLMs: 2025 Beginner‚Äôs Guide","description":"A clear, no-nonsense guide to running powerful large language models on your own hardware in 2025 ‚Äì perfect for beginners and power users alike."}]}]}]
21:["$","$L1c",null,{"children":["$","div",null,{"className":"mt-12","children":["$","div",null,{"className":"flex items-start gap-4 p-6 rounded-lg border mt-12","style":{"backgroundColor":"var(--bg-secondary)","borderColor":"var(--bg-border)"},"children":[["$","div",null,{"className":"shrink-0","children":["$","div",null,{"className":"w-16 h-16 rounded-full overflow-hidden border-2","style":{"borderColor":"var(--accent)"},"children":["$","$L25",null,{"src":"/avatar.png","alt":"BlackCatDesigns","width":64,"height":64,"className":"object-cover"}]}]}],["$","div",null,{"className":"flex-1","children":[["$","p",null,{"className":"text-sm mb-1","style":{"color":"var(--text-muted)"},"children":"Written by"}],["$","h3",null,{"className":"font-semibold mb-2","style":{"color":"var(--text-primary)"},"children":"BlackCatDesigns"}],["$","p",null,{"className":"text-sm mb-3","style":{"color":"var(--text-secondary)"},"children":"Full-stack developer passionate about building beautiful and functional web applications."}],["$","div",null,{"className":"flex gap-3","children":[["$","a",null,{"href":"https://github.com/theblackcat98","target":"_blank","rel":"noopener noreferrer","className":"transition-colors","style":{"color":"var(--text-muted)"},"aria-label":"GitHub","children":["$","svg",null,{"className":"w-5 h-5","fill":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"d":"M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"}]}]}],["$","a",null,{"href":"https://twitter.com/theblackcat98","target":"_blank","rel":"noopener noreferrer","className":"transition-colors","style":{"color":"var(--text-muted)"},"aria-label":"Twitter","children":["$","svg",null,{"className":"w-5 h-5","fill":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"d":"M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"}]}]}],"$undefined","$undefined"]}]]}]]}]}]}]
22:["$","$L1c",null,{"children":["$","div",null,{"className":"mt-12","children":null}]}]
16:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
12:null
14:{"metadata":[["$","title","0",{"children":"Getting Started with Local LLMs: 2025 Beginner‚Äôs Guide"}],["$","meta","1",{"name":"description","content":"A clear, no-nonsense guide to running powerful large language models on your own hardware in 2025 ‚Äì perfect for beginners and power users alike."}],["$","meta","2",{"property":"og:title","content":"Getting Started with Local LLMs: 2025 Beginner‚Äôs Guide"}],["$","meta","3",{"property":"og:description","content":"A clear, no-nonsense guide to running powerful large language models on your own hardware in 2025 ‚Äì perfect for beginners and power users alike."}],["$","meta","4",{"property":"og:image","content":"http://localhost:3000/api/og?title=Getting%20Started%20with%20Local%20LLMs%3A%202025%20Beginner%E2%80%99s%20Guide&description=A%20clear%2C%20no-nonsense%20guide%20to%20running%20powerful%20large%20language%20models%20on%20your%20own%20hardware%20in%202025%20%E2%80%93%20perfect%20for%20beginners%20and%20power%20users%20alike.&type=article"}],["$","meta","5",{"property":"og:type","content":"article"}],["$","meta","6",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","7",{"name":"twitter:title","content":"Getting Started with Local LLMs: 2025 Beginner‚Äôs Guide"}],["$","meta","8",{"name":"twitter:description","content":"A clear, no-nonsense guide to running powerful large language models on your own hardware in 2025 ‚Äì perfect for beginners and power users alike."}],["$","meta","9",{"name":"twitter:image","content":"http://localhost:3000/api/og?title=Getting%20Started%20with%20Local%20LLMs%3A%202025%20Beginner%E2%80%99s%20Guide&description=A%20clear%2C%20no-nonsense%20guide%20to%20running%20powerful%20large%20language%20models%20on%20your%20own%20hardware%20in%202025%20%E2%80%93%20perfect%20for%20beginners%20and%20power%20users%20alike.&type=article"}]],"error":null,"digest":"$undefined"}
19:"$14:metadata"
