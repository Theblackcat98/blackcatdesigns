1:"$Sreact.fragment"
2:I[2260,["215","static/chunks/215-388a4da7c5323a15.js","177","static/chunks/app/layout-325168dde6378f38.js"],"default"]
3:I[912,["215","static/chunks/215-388a4da7c5323a15.js","177","static/chunks/app/layout-325168dde6378f38.js"],"default"]
4:I[9766,[],""]
5:I[8924,[],""]
6:I[2619,["619","static/chunks/619-ba102abea3e3d0e4.js","113","static/chunks/app/blog/tag/%5Btag%5D/page-f746f15188dc0e70.js"],""]
7:I[979,["215","static/chunks/215-388a4da7c5323a15.js","177","static/chunks/app/layout-325168dde6378f38.js"],"default"]
10:I[7150,[],""]
:HL["/_next/static/css/3499fdfcb29162bf.css","style"]
0:{"P":null,"b":"mqoiKBG41VDaIdpl--yaH","p":"","c":["","blog","tag","local-llm",""],"i":false,"f":[[["",{"children":["blog",{"children":["tag",{"children":[["tag","local-llm","d"],{"children":["__PAGE__",{}]}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/3499fdfcb29162bf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"dark scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"href":"https://api.fontshare.com/v2/css?f[]=clash-display@400,500,600,700&f[]=satoshi@300,400,500,700,900&display=swap","rel":"stylesheet"}],["$","link",null,{"href":"https://fonts.googleapis.com/css2?family=DM+Serif+Text:ital@0;1&display=swap","rel":"stylesheet"}]]}],["$","body",null,{"className":"flex flex-col min-h-screen overflow-x-hidden","style":{"backgroundColor":"var(--bg-primary)","color":"var(--text-primary)"},"children":[["$","$L2",null,{}],["$","div",null,{"className":"fixed inset-0 pointer-events-none z-50 opacity-[0.03] mix-blend-overlay","style":{"backgroundImage":"url(\"data:image/svg+xml,%3Csvg viewBox='0 0 200 200' xmlns='http://www.w3.org/2000/svg'%3E%3Cfilter id='noiseFilter'%3E%3CfeTurbulence type='fractalNoise' baseFrequency='0.65' numOctaves='3' stitchTiles='stitch'/%3E%3C/filter%3E%3Crect width='100%25' height='100%25' filter='url(%23noiseFilter)'/%3E%3C/svg%3E\")"}}],["$","$L3",null,{}],["$","main",null,{"className":"max-w-4xl mx-auto px-6 pt-24 pb-12 flex-grow w-full","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","div",null,{"className":"flex flex-col items-center justify-center min-h-[400px] gap-6 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold","style":{"color":"var(--text-primary)"},"children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold","style":{"color":"var(--text-secondary)"},"children":"Page Not Found"}],["$","p",null,{"style":{"color":"var(--text-muted)"},"children":"The page you're looking for doesn't exist."}],["$","$L6",null,{"href":"/","className":"mt-4 px-6 py-3 font-medium transition-colors","style":{"backgroundColor":"var(--accent)","color":"var(--bg-primary)"},"children":"Back to Home"}]]}],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"border-t mt-12","style":{"borderColor":"var(--bg-border)","backgroundColor":"var(--bg-secondary)"},"children":["$","div",null,{"className":"max-w-4xl mx-auto px-6 py-12","children":[["$","div",null,{"className":"grid grid-cols-1 md:grid-cols-3 gap-8 mb-8","children":[["$","div",null,{"children":[["$","h3",null,{"className":"text-lg font-bold mb-2","style":{"color":"var(--accent)"},"children":"BlackCatDesigns"}],["$","p",null,{"className":"text-sm","style":{"color":"var(--text-muted)"},"children":"Cultivating Aesthetic Transformations"}]]}],["$","div",null,{"children":[["$","h4",null,{"className":"text-sm font-semibold mb-4","style":{"color":"var(--text-secondary)"},"children":"Navigation"}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"children":["$","$L7",null,{"href":"/","children":"Home"}]}],["$","li",null,{"children":["$","$L7",null,{"href":"/projects","children":"Projects"}]}],["$","li",null,{"children":["$","$L7",null,{"href":"/blog","children":"Blog"}]}],["$","li",null,{"children":["$","$L7",null,{"href":"/about","children":"About"}]}],["$","li",null,{"children":["$","$L7",null,{"href":"/contact","children":"Contact"}]}]]}]]}],["$","div",null,{"children":[["$","h4",null,{"className":"text-sm font-semibold mb-4","style":{"color":"var(--text-secondary)"},"children":"Connect"}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"children":["$","$L7",null,{"href":"https://github.com/theblackcat98","external":true,"children":["$","span",null,{"className":"flex items-center gap-2","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-github w-4 h-4","aria-hidden":"true","children":[["$","path","tonef",{"d":"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"}],"$L8","$undefined"]}],"GitHub"]}]}]}],"$L9"]}]]}]]}],"$La"]}]}]]}]]}]]}],{"children":["blog","$Lb",{"children":["tag","$Lc",{"children":[["tag","local-llm","d"],"$Ld",{"children":["__PAGE__","$Le",{},null,false]},null,false]},null,false]},null,false]},null,false],"$Lf",false]],"m":"$undefined","G":["$10",[]],"s":false,"S":true}
12:I[4431,[],"OutletBoundary"]
14:I[5278,[],"AsyncMetadataOutlet"]
16:I[4431,[],"ViewportBoundary"]
18:I[4431,[],"MetadataBoundary"]
19:"$Sreact.suspense"
8:["$","path","9comsn",{"d":"M9 18c-4.51 2-5-2-7-2"}]
9:["$","li",null,{"children":["$","$L7",null,{"href":"https://instagram.com/theblackcat98","external":true,"children":["$","span",null,{"className":"flex items-center gap-2","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-instagram w-4 h-4","aria-hidden":"true","children":[["$","rect","2e1cvw",{"width":"20","height":"20","x":"2","y":"2","rx":"5","ry":"5"}],["$","path","9exkf1",{"d":"M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"}],["$","line","r4j83e",{"x1":"17.5","x2":"17.51","y1":"6.5","y2":"6.5"}],"$undefined"]}],"Instagram"]}]}]}]
a:["$","div",null,{"className":"border-t pt-6 text-center text-sm","style":{"borderColor":"var(--bg-border)","color":"var(--text-muted)"},"children":["$","p",null,{"children":"Â© 2025 BlackCatDesigns. All rights reserved."}]}]
b:["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
c:["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
d:["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
e:["$","$1","c",{"children":["$L11",null,["$","$L12",null,{"children":["$L13",["$","$L14",null,{"promise":"$@15"}]]}]]}]
f:["$","$1","h",{"children":[null,[["$","$L16",null,{"children":"$L17"}],null],["$","$L18",null,{"children":["$","div",null,{"hidden":true,"children":["$","$19",null,{"fallback":null,"children":"$L1a"}]}]}]]}]
1b:I[9467,["619","static/chunks/619-ba102abea3e3d0e4.js","113","static/chunks/app/blog/tag/%5Btag%5D/page-f746f15188dc0e70.js"],"default"]
1c:T15c8,
# Getting Started with Local LLMs: 2025 Beginnerâ€™s Guide

Running large language models locally has never been more accessible. Whether youâ€™re experimenting, building private apps, or just curious, you can now get state-of-the-art performance without sending your data to the cloud.

This guide cuts through the noise and gives you exactly what you need to get started in 2025 â€“ with clear recommendations, hardware reality checks, and practical commands.

## âš¡ TL;DR â€“ The Fast Path

**Easiest start** â†’ **Ollama**  
**Maximum efficiency & control** â†’ **llama.cpp**  
**High-throughput serving** â†’ **vLLM** or **Hugging Face TGI**  
**Single-GPU API** â†’ **TabbyAPI**

**Best starter models (2025):**
- Qwen3-4B â†’ tiny but surprisingly capable
- Qwen3-14B â†’ excellent reasoning
- gpt-oss (MoE) â†’ efficiency king for long contexts

**Hardware cheat sheet (quantized):**
- 8â€“12 GB VRAM â†’ 4Bâ€“8B models
- 16â€“24 GB VRAM â†’ 14Bâ€“30B models
- 40+ GB VRAM â†’ 70B+ models

Got **20 GB VRAM + 32 GB RAM**? Youâ€™re in the sweet spot for 14B at 32k context or 4B at 128k.

## ðŸ§µ 3-Step Quick Start (Takes <10 Minutes)

```bash
# 1. Install Ollama (macOS, Windows, Linux)
# Visit https://ollama.com and download

# 2. Pull and run a great starter model
ollama pull qwen3:4b
ollama run qwen3:4b

# 3. Want longer context? (if the model supports YaRN/RoPE scaling)
ollama run qwen3:4b --num_ctx 32768
```

Thatâ€™s it. Youâ€™re now running a local LLM.

## ðŸ–¥ Choose Your Inference Engine

| Tool              | Best For                          | Difficulty | Notes                                      |
|-------------------|-----------------------------------|------------|--------------------------------------------|
| **Ollama**        | Beginners, quick chat, prototyping| â˜…â˜†â˜†â˜†â˜†     | Simple CLI + API, works with OpenWebUI     |
| **llama.cpp**     | Max speed & control, edge devices | â˜…â˜…â˜†â˜†â˜†     | Extremely efficient, huge community        |
| **vLLM**          | High-throughput GPU serving       | â˜…â˜…â˜…â˜†â˜†     | PagedAttention = more tokens/sec           |
| **Hugging Face TGI** | Production-grade API servers    | â˜…â˜…â˜…â˜†â˜†     | OpenAI-compatible, great multi-GPU         |
| **TabbyAPI**      | Lightweight single-GPU API        | â˜…â˜…â˜†â˜†â˜†     | Fast setup, works great with OpenWebUI     |
| **TensorRT-LLM**  | Peak NVIDIA performance           | â˜…â˜…â˜…â˜…â˜†     | Complex but fastest on RTX 40/H100         |

Start with **Ollama**. Graduate to **llama.cpp** or **vLLM** when you outgrow it.

## ðŸ–¼ Recommended Frontends (Optional but Nice)

- **OpenWebUI** â€“ Beautiful browser interface (works with Ollama, TabbyAPI, vLLM)
- **LM Studio** â€“ Excellent model manager + local OpenAI-compatible server
- **Jan** â€“ Clean, cross-platform, open-source

## âš™ï¸ Hardware Reality Check (2025)

| Resource     | Minimum          | Recommended         | Notes                                      |
|--------------|------------------|---------------------|--------------------------------------------|
| GPU          | RTX 3060 12 GB   | RTX 4090 / A6000+   | NVIDIA dominates local inference           |
| VRAM         | 8 GB             | 24 GB+              | The #1 bottleneck                          |
| System RAM   | 16 GB            | 32â€“64 GB            | Needed for KV cache spillover              |
| Storage      | 50 GB free       | NVMe SSD            | Quantized models: 2â€“50 GB each             |

**Pro tip**: Mixture-of-Experts (MoE) models like Mixtral 8x7B activate only 7B parameters per token, gpt-oss activates ~1.7B â†’ they punch way above their weight on hybrid CPU+GPU setups.

## ðŸ“– Quick Vocabulary (Youâ€™ll Hear These Terms)

- **GGUF** â€“ The go-to format for quantized models (used by Ollama & llama.cpp)
- **Quantization** â€“ Compressing model weights (Q4_K_M = great balance)
- **Context** â€“ How much text the model can â€œseeâ€ at once
- **KV Cache** â€“ Memory that grows with context length (VRAM eater)
- **YaRN / RoPE Scaling** â€“ Tricks to extend context beyond original training
- **MoE** â€“ Only a fraction of the model runs per token â†’ efficient
- **PagedAttention** â€“ vLLMâ€™s secret sauce for high throughput

## ðŸ“ Context vs VRAM Cheat Sheet (Q4_K_M Quantized)

| Model              | 32k Context | 64k Context | 128k Context | Notes                              |
|--------------------|-------------|-------------|--------------|------------------------------------|
| Qwen3-4B           | 5â€“6 GB      | 10â€“12 GB    | 20â€“24 GB     | Perfect for modest GPUs            |
| Qwen3-14B          | 16â€“20 GB    | 32â€“38 GB    | Not advised  | Sweet spot at 32k                  |
| gpt-oss (MoE) | 10â€“12 GB    | 20â€“24 GB    | 40â€“48 GB     | Best efficiency for long contexts  |

## ðŸ”§ Practical Tips for Your Rig (20 GB VRAM + 32 GB RAM Example)

- Run **Qwen3-14B** comfortably at 32k context
- Use **Qwen3-4B** when you need 64kâ€“128k
- Pick **gpt-oss** for the best long-context efficiency
- Default to **Q4_K_M** quantization â€“ best quality/size tradeoff
- Most backends support OpenAI-compatible APIs â†’ swap tools by changing a URL

## âœ… Your Next Steps

1. Install **Ollama** (or your chosen engine)
2. Try **qwen3:4b** or **qwen3:14b**
3. Experiment with `--num_ctx` for longer context
4. Add **OpenWebUI** or **LM Studio** for a nicer experience

Local LLMs are evolving fast â€“ new models, better quantization, and longer contexts drop almost monthly. Bookmark this guide and check back.

11:["$","div",null,{"className":"space-y-8","children":[["$","nav",null,{"className":"flex items-center gap-2 text-sm","style":{"color":"var(--text-muted)"},"children":[["$","$L6",null,{"href":"/","className":"transition-colors","style":{"color":"var(--text-muted)"},"children":"Home"}],["$","span",null,{"children":"â€º"}],["$","$L6",null,{"href":"/blog","className":"transition-colors","style":{"color":"var(--text-muted)"},"children":"Blog"}],["$","span",null,{"children":"â€º"}],["$","span",null,{"style":{"color":"var(--accent)"},"children":["Tag: ","local-llm"]}]]}],["$","div",null,{"children":[["$","h1",null,{"className":"text-4xl font-bold mb-2","style":{"color":"var(--text-primary)"},"children":["Posts tagged \"","local-llm","\""]}],["$","p",null,{"style":{"color":"var(--text-secondary)"},"children":[1," post",""," found"]}]]}],["$","$L1b",null,{"posts":[{"slug":"llm-starter-guide","content":"$1c","title":"Getting Started with Local LLMs: 2025 Beginnerâ€™s Guide","date":"2025-11-28","description":"A clear, no-nonsense guide to running powerful large language models on your own hardware in 2025 â€“ perfect for beginners and power users alike.","author":"theblackcat","tags":["local-llm","ai","ollama","llama-cpp","self-hosted","2025"],"published":true,"readingTime":4,"coverImage":""}],"allCategories":["2025","ai","llama-cpp","local-llm","ollama","self-hosted"]}]]}]
17:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
13:null
15:{"metadata":[["$","title","0",{"children":"Posts tagged \"local-llm\""}],["$","meta","1",{"name":"description","content":"All blog posts tagged with local-llm"}]],"error":null,"digest":"$undefined"}
1a:"$15:metadata"
